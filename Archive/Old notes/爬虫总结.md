#  爬虫总结

##  一、 requests

### 1. 静态页面爬取

使用 `requests`库的 `get()`方法获取到网页的源文件，在通过正则表达式或xpath获取到所需数据。

### 2. 动态数据爬取

通过浏览器开发工具找到页面上获取数据的请求接口，分析接口参数，然后使用`requests`库发送相应的请求，获取到数据

## 二、.Scrapy

### 1. 静态页面爬取

通过 `scrapy`框架定义 `item`，然后编写`spider`代码，使用`xpath`获取到数据，再通过`pipeline`（管道）存储数据

### 2. 动态数据爬取

去掉  spider 的`start_urls`  参数，重写 `start_requests(self) `方法，在此方法内进行接口的请求 

### 3. 翻页查询

#### 静态翻页：

在 spider 程序内的 parse()方法中先编写爬取一页的程序，yield item；再利用xpath()获取到下一页的连接。最后使用 scrapy.Request(url,callback=self.parse) 进行请求后回调 paese()方法。

实例：

```python
    def parse(self, response):
        
        article_list = response.xpath('//div[@class = "card-content article"]')
        for article in article_list:
            item = SkyzcItem()

            name = article.xpath("./h1/a/text()").extract()
            abstract = article.xpath("./div[@class = 'content']/text()").extract()
            label = article.xpath("./div/div/div/a/text()").extract()
            time = article.xpath("./div/div/time/text()").extract()

            item['name'] = name[0]
            item['abstract'] = abstract[0]
            item['label'] = label[0]
            item['time'] = time[0]

            yield item

        # yield self.get_info(response)
        # 获取到“下一页” url
        next_url = response.xpath("//div[@class='pagination-next']/a/@href").extract()
        if next_url:
            print('有下一页喔'+next_url[0])
            # 再次发送请求，并回调 parse()方法
            yield scrapy.Request(next_url[0],callback=self.parse)
```





## 三、文件操作

